# -*- coding: utf-8 -*-
"""Structured_DB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPgh3ciYwuq6i7NLIvzYeRt2wHO9122M
"""

!pip install neo4j

import os
import re
import uuid
import hashlib
from typing import Dict, Any, Optional

import pandas as pd
from dateutil.parser import parse as parse_date
from neo4j import GraphDatabase

# ---------------------------
# Config
# ---------------------------
ENTITIES_CSV = "/content/entities (1).csv"
COUNTRY_CODES_CSV = "/content/country_codes_cleaned.csv"
ATTACKS_CSV = "/content/pirate_attacks_final_cleaned.csv"

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://0.tcp.in.ngrok.io:13018")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "EntitiesDB")
DB_NAME = os.getenv("NEO4J_DATABASE", None)  # set if your default isn't "neo4j"

# === Transformer RE Config ===
USE_TRANSFORMER_RE = False  # safer default
RE_MODEL_ID = "Babelscape/rebel-large"
re_pipeline = None  # lazy-loaded

MIN_DESC_LEN_FOR_RE = 25

# ---------------------------
# Utility helpers
# ---------------------------

def norm(s: Optional[str]) -> Optional[str]:
    if s is None:
        return None
    if isinstance(s, float):
        if pd.isna(s):
            return None
        s = str(s)
    s = str(s).strip()
    if not s or s.lower() in {"nan", "none", "null", "unknown"}:
        return None
    return s

def to_float(x) -> Optional[float]:
    """Robust numeric cast: returns None for NaN/empty/invalid."""
    if x is None:
        return None
    if isinstance(x, str):
        if not x.strip():
            return None
    try:
        f = float(x)
    except Exception:
        return None
    if pd.isna(f):
        return None
    return f

def safe_date(s: Optional[str]) -> Optional[str]:
    s = norm(s)
    if not s:
        return None
    try:
        return parse_date(s, fuzzy=True).date().isoformat()
    except Exception:
        return None

def attack_id_from_row(row: pd.Series) -> str:
    """
    Stable (but unique) ID composed from date, coords, vessel_name.
    If coords are missing, fall back to a hash to avoid collisions.
    """
    date_iso = safe_date(row.get("date"))
    vessel = norm(row.get("vessel_name"))
    lon = to_float(row.get("longitude"))
    lat = to_float(row.get("latitude"))

    parts = [p for p in [date_iso, vessel] if p]
    if lon is not None and lat is not None:
        parts.append(f"{lon:.4f},{lat:.4f}")

    base = "|".join(parts)
    if base:
        # Add a short hash of the entire row to prevent collisions among identical bases
        row_fingerprint = hashlib.sha1(
            ("|".join([str(k)+":"+str(row.get(k)) for k in sorted(row.index)]))
            .encode("utf-8")
        ).hexdigest()[:8]
        return f"{base}|{row_fingerprint}"
    return f"attack:{uuid.uuid4()}"

# ---------------------------
# Optional rule-based RE from text
# ---------------------------
RE_PATTERNS = [
    (re.compile(r"\b(hijack(?:ed|ing)?)\b", re.I), ("HAS_EVENT", "Hijacking")),
    (re.compile(r"\b(board(?:ed|ing))\b", re.I), ("HAS_EVENT", "Boarding")),
    (re.compile(r"\b(robbery|robbed)\b", re.I), ("HAS_EVENT", "Robbery")),
    (re.compile(r"\b(kidnap(?:ped|ping)?)\b", re.I), ("HAS_EVENT", "Kidnapping")),
    (re.compile(r"\b(fire(?:d)? (?:upon|on)|shots? fired)\b", re.I), ("HAS_EVENT", "FiredUpon")),
    (re.compile(r"\b(crew)\b", re.I), ("MENTIONS", "Crew")),
]

def extract_text_relations(desc: str):
    rels = set()
    if not desc:
        return []
    for pat, rel in RE_PATTERNS:
        if pat.search(desc):
            rels.add(rel)
    return list(rels)

# ---------------------------
# Transformer triple extraction (lazy)
# ---------------------------
RELATION_MAP = {
    "founded by": "FOUNDED_BY",
    "located in": "LOCATED_IN",
    "part of": "PART_OF",
}

RELATION_SAFE = re.compile(r"^[A-Z_][A-Z0-9_]*$")

def get_re_pipeline():
    global re_pipeline
    if re_pipeline is not None:
        return re_pipeline
    # Lazy import so script still runs without transformers installed
    try:
        from transformers import pipeline as hf_pipeline
        re_pipeline = hf_pipeline("text2text-generation", model=RE_MODEL_ID)
    except Exception as e:
        print(f"[WARN] Could not load transformers pipeline ({e}). Disabling transformer RE.")
        re_pipeline = None
    return re_pipeline

def extract_triples_from_text(text: Optional[str]):
    triples = []
    text = norm(text)
    if not text:
        return triples
    pipe = get_re_pipeline()
    if not pipe:
        return triples
    try:
        outputs = pipe(text, max_length=512)
    except Exception as e:
        print(f"[WARN] Transformer RE failed: {e}")
        return triples

    if not outputs:
        return triples

    generated_text = outputs[0].get("generated_text", "")
    # REBEL format: "<triplet> head | relation | tail </triplet>"
    for triple_str in generated_text.split("<triplet>"):
        if "</triplet>" in triple_str:
            parts = triple_str.split("|")
            if len(parts) == 3:
                head = parts[0].strip().replace("</triplet>", "")
                relation = parts[1].strip().replace("</triplet>", "")
                tail = parts[2].strip().replace("</triplet>", "")
                triples.append((head, relation, tail))
    return triples

def sanitize_relation_label(rel: str) -> Optional[str]:
    if not rel:
        return None
    rel = rel.replace(" ", "_").upper()
    if RELATION_SAFE.match(rel):
        return rel
    return None

# ---------------------------
# Neo4j driver & schema
# ---------------------------
class Neo4jKG:
    def __init__(self, uri: str, user: str, password: str, database: Optional[str] = None, encrypted: bool = False):
        # encrypted=False is typical when tunneling bolt:// over ngrok TCP
        self.driver = GraphDatabase.driver(uri, auth=(user, password), encrypted=encrypted)
        self.database = database

    def close(self):
        self.driver.close()

    def _session(self):
        if self.database:
            return self.driver.session(database=self.database)
        return self.driver.session()

    def init_constraints(self):
        cypher = [
            "CREATE CONSTRAINT country_code IF NOT EXISTS FOR (c:Country) REQUIRE c.code IS UNIQUE",
            "CREATE CONSTRAINT vessel_name IF NOT EXISTS FOR (v:Vessel) REQUIRE v.name IS UNIQUE",
            "CREATE CONSTRAINT vtype_name IF NOT EXISTS FOR (t:VesselType) REQUIRE t.name IS UNIQUE",
            "CREATE CONSTRAINT atype_name IF NOT EXISTS FOR (t:AttackType) REQUIRE t.name IS UNIQUE",
            "CREATE CONSTRAINT wbody_name IF NOT EXISTS FOR (w:Waterbody) REQUIRE w.name IS UNIQUE",
            "CREATE CONSTRAINT source_name IF NOT EXISTS FOR (s:DataSource) REQUIRE s.name IS UNIQUE",
            "CREATE CONSTRAINT date_iso IF NOT EXISTS FOR (d:Date) REQUIRE d.iso IS UNIQUE",
            "CREATE CONSTRAINT attack_id IF NOT EXISTS FOR (a:Attack) REQUIRE a.attack_id IS UNIQUE",
        ]
        with self._session() as sess:
            for q in cypher:
                sess.run(q)

    def upsert_country(self, code: str, name: Optional[str], region: Optional[str]):
        query = (
            "MERGE (c:Country {code:$code}) "
            "SET c.name = coalesce($name, c.name), c.region = coalesce($region, c.region)"
        )
        with self._session() as sess:
            sess.run(query, code=code, name=name, region=region)

    def upsert_attack_graph(self, row: Dict[str, Any]):
        # Prepare fields
        attack_id = row["attack_id"]
        date_iso = row.get("date_iso")
        time_val = norm(row.get("time"))
        lon = to_float(row.get("longitude"))
        lat = to_float(row.get("latitude"))
        eez_code = norm(row.get("eez_country"))
        near_code = norm(row.get("nearest_country"))
        wb_name = norm(row.get("location_description"))
        vessel_name = norm(row.get("vessel_name"))
        vessel_type = norm(row.get("vessel_type"))
        attack_type = norm(row.get("attack_type"))
        data_source = norm(row.get("data_source"))
        v_status = norm(row.get("vessel_status"))
        shore_dist = to_float(row.get("shore_distance"))
        shore_lon = to_float(row.get("shore_longitude"))
        shore_lat = to_float(row.get("shore_latitude"))
        desc = norm(row.get("attack_description"))

        params = {
            "attack_id": attack_id,
            "date_iso": date_iso,
            "time_val": time_val,
            "lon": lon,
            "lat": lat,
            "eez_code": eez_code,
            "near_code": near_code,
            "wb_name": wb_name,
            "vessel_name": vessel_name,
            "vessel_type": vessel_type,
            "attack_type": attack_type,
            "data_source": data_source,
            "v_status": v_status,
            "shore_distance": shore_dist,
            "shore_lon": shore_lon,
            "shore_lat": shore_lat,
            "desc": desc,
        }

        cypher_parts = []

        # Attack node
        cypher_parts.append(
            "MERGE (a:Attack {attack_id:$attack_id}) "
            "SET a.date = $date_iso, a.time = $time_val, a.longitude = $lon, a.latitude = $lat, "
            "    a.vessel_status = $v_status, a.description = $desc, "
            "    a.shore_distance = $shore_distance, a.shore_longitude = $shore_lon, a.shore_latitude = $shore_lat"
        )

        # Date
        if date_iso:
            cypher_parts.append(
                "WITH a "
                "MERGE (d:Date {iso:$date_iso}) "
                "MERGE (a)-[:ON_DATE]->(d)"
            )

        # EEZ country
        if eez_code:
            cypher_parts.append(
                "WITH a "
                "MATCH (c:Country {code:$eez_code}) "
                "MERGE (a)-[:WITHIN_EEZ]->(c)"
            )

        # Nearest country
        if near_code:
            cypher_parts.append(
                "WITH a "
                "MATCH (c2:Country {code:$near_code}) "
                "MERGE (a)-[:OCCURRED_NEAR]->(c2)"
            )

        # Waterbody / location description
        if wb_name:
            cypher_parts.append(
                "WITH a "
                "MERGE (w:Waterbody {name:$wb_name}) "
                "MERGE (a)-[:LOCATED_IN]->(w)"
            )

        # Vessel and type
        if vessel_name:
            cypher_parts.append(
                "WITH a "
                "MERGE (v:Vessel {name:$vessel_name}) "
                "MERGE (a)-[:TARGETED]->(v)"
            )
            if vessel_type:
                cypher_parts.append(
                    "WITH v "
                    "MERGE (vt:VesselType {name:$vessel_type}) "
                    "MERGE (v)-[:HAS_TYPE]->(vt)"
                )

        # Attack type
        if attack_type:
            cypher_parts.append(
                "WITH a "
                "MERGE (at:AttackType {name:$attack_type}) "
                "MERGE (a)-[:HAS_TYPE]->(at)"
            )

        # Data source
        if data_source:
            cypher_parts.append(
                "WITH a "
                "MERGE (s:DataSource {name:$data_source}) "
                "MERGE (a)-[:RECORDED_IN]->(s)"
            )

        cypher = "\n".join(cypher_parts)
        with self._session() as sess:
            sess.run(cypher, **params)

        # Optional: rule-based extra relations from description
        if desc and len(desc) >= MIN_DESC_LEN_FOR_RE:
            extra = extract_text_relations(desc)
            for rel, label in extra:
                with self._session() as sess:
                    sess.run(
                        "MATCH (a:Attack {attack_id:$attack_id}) "
                        "MERGE (e:EventTag {name:$label}) "
                        f"MERGE (a)-[:{rel}]->(e)",
                        {"attack_id": attack_id, "label": label},
                    )

# ---------------------------
# Main ETL
# ---------------------------

# Load CSVs
entities = pd.read_csv(ENTITIES_CSV)
attacks = pd.read_csv(ATTACKS_CSV)
country_codes = pd.read_csv(COUNTRY_CODES_CSV)

# Normalize countries from entities.csv
country_rows = entities[
    entities["entity_type"].astype(str).str.strip().str.lower().str.contains("country", na=False)
]

countries: Dict[str, Dict[str, Optional[str]]] = {}
for eid, grp in country_rows.groupby("entity_id"):
    rec: Dict[str, Optional[str]] = {"code": None, "name": None, "region": None}
    for _, r in grp.iterrows():
        key = str(r.get("property_key", "")).lower()
        val = norm(r.get("property_value"))
        if key == "code":
            rec["code"] = val
        elif key == "name":
            rec["name"] = val
        elif key == "region":
            rec["region"] = val
    if rec["code"]:
        countries[rec["code"]] = rec

kg = Neo4jKG(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, database=DB_NAME, encrypted=False)

try:
    kg.init_constraints()

    # Step 1: Upsert countries
    for code, info in countries.items():
        kg.upsert_country(code, info.get("name"), info.get("region"))

    # Step 2: Enrich with country_codes
    for _, row in country_codes.iterrows():
        code = str(row.get("code", "")).strip()
        name = str(row.get("name", "")).strip()
        if code and name:
            with kg._session() as sess:
                sess.run(
                    "MERGE (c:Country {code: $code}) "
                    "SET c.name = $name",
                    {"code": code, "name": name},
                )

    # Step 3: Load attacks
    # (Do NOT fillna("") globally; keep NaNs)
    if USE_TRANSFORMER_RE:
        get_re_pipeline()  # try to warm up; if it fails, it will disable itself

    for _, row in attacks.iterrows():
        row_dict = row.to_dict()
        row_dict["attack_id"] = attack_id_from_row(row)
        row_dict["date_iso"] = safe_date(row.get("date"))
        kg.upsert_attack_graph(row_dict)

        # Optional REBEL triples
        if USE_TRANSFORMER_RE and re_pipeline:
            triples = extract_triples_from_text(row.get("attack_description", ""))
            for head, relation, tail in triples:
                rel = RELATION_MAP.get(relation.lower(), relation.replace(" ", "_").upper())
                rel = sanitize_relation_label(rel)
                if not rel:
                    continue
                with kg._session() as sess:
                    sess.run(
                        f"MERGE (h:Entity {{name:$head}}) "
                        f"MERGE (t:Entity {{name:$tail}}) "
                        f"MERGE (h)-[:{rel}]->(t)",
                        {"head": head, "tail": tail},
                    )

finally:
    kg.close()

!python --version

